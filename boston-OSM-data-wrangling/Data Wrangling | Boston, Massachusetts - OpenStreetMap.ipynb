{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Area\n",
    "Boston, MA, USA\n",
    "- [Boston on OpenStreetMap](https://www.openstreetmap.org/relation/2315704)\n",
    "- [Boston Metro extract](https://mapzen.com/data/metro-extracts/metro/boston_massachusetts/)\n",
    "\n",
    "This is the city that welcomed we to the States, where I studied and graduated. I have a particular fondness for Boston, and I'm interested in seeing how good the data is and if it can be improved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to wrangle the Boston data from OpenStreetMap. This means that we're going to:\n",
    "- **gather**\n",
    "- **extract**\n",
    "- **clean**\n",
    "- **store** our data\n",
    "in order to prepare it for analysis.  \n",
    "\n",
    "The data we have is in `xml` format. We will export it in `csv` and store it in an `SQL` database.\n",
    "\n",
    "In order to clean the data, we will audit its quality according to these five characteristics:\n",
    "- **validity**\n",
    "- **accuracy**\n",
    "- **completeness**\n",
    "- **consistency**\n",
    "- **uniformity**\n",
    "\n",
    "Precisely, we might have to perform some of these tasks:\n",
    "- remove or correct **typographical errors**\n",
    "- validate against **other entities**\n",
    "- **cross check** with another dataset\n",
    "- data **enhancement** (make the data more complete)\n",
    "- data **harmonization**\n",
    "- change **reference** data\n",
    "\n",
    "Most of the time, we will define helper functions so that they can be reused, here or in another project, and save us some time.  \n",
    "\n",
    "Once all of this is done, we will export our cleaned data into csv files, and import them into an SQL database to perform further analytics.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has already been done. The exact dataset can be downloaded [here](https://s3.amazonaws.com/metro-extracts.mapzen.com/boston_massachusetts.osm.bz2) (although it is certain that this dataset will evolve in the future).  \n",
    "It has been unzipped and stored in the `data` folder of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to extract the data so we can start working with it.\n",
    "The first thing to do is to look at the `boston_massachusetts.osm` file, and get a sense of its structure.  \n",
    "It follows an XML structure, humanly readable. The way it works is simple. There are several tags, and we're interested in three in particular:\n",
    "- [nodes](http://wiki.openstreetmap.org/wiki/Node), which consist of \"a **single point in space** defined by its *latitude*, *longitude* and *node id*. Nodes can be used to define standalone point features. In this case, a node will normally have at least one tag to define its purpose. Nodes are often used to define the shape or \"path\" of a way.\"\n",
    "- [ways](http://wiki.openstreetmap.org/wiki/Way), which are an \"**ordered list of nodes** and normally also has at least one tag\". To make it simple, these are streets, avenues...\n",
    "- [relations](http://wiki.openstreetmap.org/wiki/Relation), which are \"used to model logical (and usually local) or **geographic relationships between objects**\". To make it simple, these are areas made of several ways.  \n",
    "\n",
    "**Nodes** are represented with the tag `<node></node>`. They can have the following attributes:\n",
    "- `id` - integer, the unique ID of the node\n",
    "- `lat` - integer, the latitude\n",
    "- `lon` - integer, the longitude\n",
    "- `version` - integer, the number of edits\n",
    "- `timestamp` - W3C datetime format, time of the last modification\n",
    "- `changeset` - integer, the changeset number in which the object was created or updated. A changeset consists of a edits made by a single user over a short period of time\n",
    "- `uid` - integer, the unique ID of the user who last edited\n",
    "- `user` - string, the pseudonym of the user who last edited\n",
    "  \n",
    "**Ways** are trepresented with the tag `<way></way>`. They can have the following attributes:\n",
    "- `id` - integer, the unique ID of the node\n",
    "- `version` - integer, the number of edits\n",
    "- `timestamp` - W3C datetime format, time of the last modification\n",
    "- `changeset` - integer, the changeset number in which the object was created or updated. A changeset consists of a edits made by a single user over a short period of time\n",
    "- `uid` - integer, the unique ID of the user who last edited\n",
    "- `user` - string, the pseudonym of the user who last edited\n",
    "\n",
    "**Relations** are represented with the tag `<relation></relation>`. They can have the same attributes as ways.\n",
    "\n",
    "**Nodes** that make up the **ways** are represented with the tag `<nd />`. They can only have a `ref` attribute, which basically references the `id` of the node as an integer.\n",
    "\n",
    "**Ways** and **nodes** in **relations** are represented with the tag `<member />`. They can have three attributes:\n",
    "- `type` - string, the type of the object (node, way)\n",
    "- `ref` - integer, the references to the unique ID of the object\n",
    "- `role` - string, the role it plays in the relation (`via`, `from`, `to`)\n",
    "\n",
    "**Tags** are represented with the tag `<tag />`. They are used as child elements of nodes, ways and relations to bring additional information. They can have two attributes:\n",
    "- `k` - string, the key of the tag\n",
    "- `v` - string, the value of the tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, were going to get to know our dataset, its **436.2 MB** of data and its **5,920,723** lines of code.  \n",
    "Let's find out exactly **what tags** we have, and **how many** of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1, 'node': 1939872, 'tag': 907379, 'nd': 2335749, 'way': 310285, 'member': 10894, 'relation': 1263, 'osm': 1}\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "\n",
    "dataset = \"data/boston_massachusetts.osm\"\n",
    "\n",
    "def count_tags(filename):\n",
    "    \"\"\"\n",
    "    Takes in a dataset in XML format.\n",
    "    Returns a dictionary of the tags and the number of each tag.\n",
    "    \"\"\"\n",
    "    tag_dict = {}\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if elem.tag not in tag_dict:\n",
    "            tag_dict[elem.tag] = 1\n",
    "        else:\n",
    "            tag_dict[elem.tag] += 1\n",
    "\n",
    "    return tag_dict\n",
    "\n",
    "print(count_tags(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there isn't so many different tags. We now know that for this dataset, we have:\n",
    "- 1,939,872 nodes\n",
    "- 310,285 ways\n",
    "- 1,263 relations\n",
    "- 10,894 members\n",
    "- 2,335,749 nds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's tackle the users question right away:  \n",
    "**how many unique users** do we have to thank for creating and updating this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1402\n"
     ]
    }
   ],
   "source": [
    "def get_users(filename):\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        for att in element.attrib:\n",
    "            if att == 'uid':\n",
    "                if element.attrib['uid'] not in users:\n",
    "                    users.add(element.attrib['uid'])\n",
    "\n",
    "    return users\n",
    "\n",
    "print(len(get_users(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to thank 1,402 people for making this dataset possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from looking at the data, we already know we're going to have some potential problems. All the `<tag />` elements don't necessarily share the same format. Some are **lowercase**, others are **uppercase**; some are just **text**, others have one or more **colon**.  \n",
    "\n",
    "This is going to be problematic when trying to expand them to a reusable dictionary. Let's see how many of each we have, using **regular expressions**, and make a list of values for each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 793897, 'lower_colon': 74286, 'problemchars': 1, 'other': 39195}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    \"\"\"\n",
    "    Takes in an XML element (<node />, <way />) and a dictionary of keys.\n",
    "    If the element is a <tag />, its ['k'] attribute is analyzed,\n",
    "    the dictionary keys and lists are incremented accordingly,\n",
    "    - lower: valid tags that only contain lowercase letters\n",
    "    - lower_colon: valid tags that contain lowercase letter with one or more colons in their name\n",
    "    - problemchars: tags with problematic characters\n",
    "    - other: tags that don't fall in the previous three categories\n",
    "    \"\"\"\n",
    "    \n",
    "    if element.tag == 'tag':\n",
    "        if lower.search(element.attrib['k']):\n",
    "            keys['lower'] +=1\n",
    "        elif lower_colon.search(element.attrib['k']):\n",
    "            keys['lower_colon'] += 1\n",
    "        elif problemchars.search(element.attrib['k']):\n",
    "            keys['problemchars'] += 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_tags(filename):\n",
    "    \"\"\"\n",
    "    Takes in a dataset in XML format, parses it, and executes the function key_type() for each element.\n",
    "    Returns a dictionary with the count of different types of keys.\n",
    "    \"\"\"\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "tag_type_dict = process_tags(dataset)\n",
    "print (tag_type_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apparently have one tag that has problematic keys. Let's find out which one it is. We know we only have one in this dataset, but for convenience, we'll have the function return a list in case other datasets have more than one problematic key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['service area']\n"
     ]
    }
   ],
   "source": [
    "def get_problemkeys(filename):\n",
    "    \"\"\"\n",
    "    Takes in a dataset in XML format, parses it and returns a list with the values of tags with problematic characters.\n",
    "    \"\"\"\n",
    "    problemchars_list = []\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if element.tag == 'tag':\n",
    "            if problemchars.search(element.attrib['k']):\n",
    "                problemchars_list.append(element.attrib['k'])\n",
    "    return problemchars_list\n",
    "\n",
    "print(get_problemkeys(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's just one with a space. Shouldn't be an issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get to cleaning the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to audit the data to get all the different street types it contains. Then we will be able to build a dictionary upon which to base our correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'Avenue': 851, 'Street': 4137, 'Drive': 77, 'Parkway': 27, 'street': 2, 'Road': 248, 'Ave': 163, 'Ave.': 21, 'St': 252, 'St,': 1, 'Broadway': 98, 'Garage': 1, 'Place': 83, 'Lane': 27, 'Pasteur': 3, 'Square': 68, 'St.': 42, 'Boulevard': 12, 'Rd': 29, 'Way': 25, 'Pkwy': 10, 'Ct': 8, 'Fellsway': 15, 'Wharf': 5, 'Elm': 1, 'Highway': 13, 'Hwy': 1, 'Artery': 1, 'Center': 15, 'Newbury': 2, 'Holland': 1, 'Lafayette': 1, 'Street.': 1, 'Floor': 1, '1702': 1, '6': 1, 'South': 2, 'LEVEL': 1, 'rd.': 1, 'Greenway': 2, 'Corner': 1, 'Row': 6, 'Sq.': 1, '303': 1, 'floor': 2, '1100': 1, '846028': 1, 'Hall': 1, 'Turnpike': 2, 'st': 1, 'Park': 52, 'Terrace': 17, 'Jamaicaway': 2, 'Plaza': 2, 'Court': 9, 'Building': 1, 'Market': 1, '#1302': 1, '#12': 1, '#501': 1, 'Dartmouth': 1, '104': 1, 'Yard': 1, 'Mall': 3, 'Boylston': 2, 'Driveway': 1, 'Winsor': 1, 'ST': 1, 'Cambrdige': 2, 'Albany': 3, 'Fenway': 5, 'HIghway': 1, 'Windsor': 2, 'Ext': 1, 'Circle': 7, 'place': 1, 'Pl': 1, 'Brook': 1, 'Hampshire': 1, 'Longwood': 1, 'Dr': 1, '3': 1, 'H': 1}) %s: %d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'#12': 1,\n",
       "             '#1302': 1,\n",
       "             '#501': 1,\n",
       "             '104': 1,\n",
       "             '1100': 1,\n",
       "             '1702': 1,\n",
       "             '3': 1,\n",
       "             '303': 1,\n",
       "             '6': 1,\n",
       "             '846028': 1,\n",
       "             'Albany': 3,\n",
       "             'Artery': 1,\n",
       "             'Ave': 163,\n",
       "             'Ave.': 21,\n",
       "             'Avenue': 851,\n",
       "             'Boulevard': 12,\n",
       "             'Boylston': 2,\n",
       "             'Broadway': 98,\n",
       "             'Brook': 1,\n",
       "             'Building': 1,\n",
       "             'Cambrdige': 2,\n",
       "             'Center': 15,\n",
       "             'Circle': 7,\n",
       "             'Corner': 1,\n",
       "             'Court': 9,\n",
       "             'Ct': 8,\n",
       "             'Dartmouth': 1,\n",
       "             'Dr': 1,\n",
       "             'Drive': 77,\n",
       "             'Driveway': 1,\n",
       "             'Elm': 1,\n",
       "             'Ext': 1,\n",
       "             'Fellsway': 15,\n",
       "             'Fenway': 5,\n",
       "             'Floor': 1,\n",
       "             'Garage': 1,\n",
       "             'Greenway': 2,\n",
       "             'H': 1,\n",
       "             'HIghway': 1,\n",
       "             'Hall': 1,\n",
       "             'Hampshire': 1,\n",
       "             'Highway': 13,\n",
       "             'Holland': 1,\n",
       "             'Hwy': 1,\n",
       "             'Jamaicaway': 2,\n",
       "             'LEVEL': 1,\n",
       "             'Lafayette': 1,\n",
       "             'Lane': 27,\n",
       "             'Longwood': 1,\n",
       "             'Mall': 3,\n",
       "             'Market': 1,\n",
       "             'Newbury': 2,\n",
       "             'Park': 52,\n",
       "             'Parkway': 27,\n",
       "             'Pasteur': 3,\n",
       "             'Pkwy': 10,\n",
       "             'Pl': 1,\n",
       "             'Place': 83,\n",
       "             'Plaza': 2,\n",
       "             'Rd': 29,\n",
       "             'Road': 248,\n",
       "             'Row': 6,\n",
       "             'ST': 1,\n",
       "             'South': 2,\n",
       "             'Sq.': 1,\n",
       "             'Square': 68,\n",
       "             'St': 252,\n",
       "             'St,': 1,\n",
       "             'St.': 42,\n",
       "             'Street': 4137,\n",
       "             'Street.': 1,\n",
       "             'Terrace': 17,\n",
       "             'Turnpike': 2,\n",
       "             'Way': 25,\n",
       "             'Wharf': 5,\n",
       "             'Windsor': 2,\n",
       "             'Winsor': 1,\n",
       "             'Yard': 1,\n",
       "             'floor': 2,\n",
       "             'place': 1,\n",
       "             'rd.': 1,\n",
       "             'st': 1,\n",
       "             'street': 2})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "street_type_re = re.compile(r'\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(int)\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        street_types[street_type] += 1\n",
    "\n",
    "def print_sorted_dict(d, expression):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print (expression % (k, v))\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit(filename):\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])\n",
    "    print(street_types, \"%s: %d\")\n",
    "    return(street_types)\n",
    "\n",
    "all_types = audit(dataset)\n",
    "all_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a number of issues here:\n",
    "- we've got a lot of numbers that should probably correspond to `addr:housenumber`.\n",
    "- we've got a lot of different abbreviations for different street types\n",
    "- we've got names of streets and parks instead of the type: Boylston is a Street, Broadway is a highway, Fenway is a park, and so on\n",
    "- we've got some typos: 'HIghway', 'Cambrdige'...\n",
    "\n",
    "Let's tackle these issues one at a time. First, we will correct the abbraviations issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting abbreviations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a dictionary of the street types we deem valid, and of the corrections we want to implement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected = ['Artery', 'Avenue', 'Boulevard', 'Broadway', 'Commons', 'Court', 'Drive', 'Lane', 'Park', 'Parkway',\n",
    "            'Place', 'Road', 'Square', 'Street', 'Terrace', 'Trail', 'Turnpike', 'Wharf',\n",
    "            'Yard']\n",
    "\n",
    "abbr_mapping = { 'Ave': 'Avenue',\n",
    "                  'Ave.': 'Avenue',\n",
    "                  'Ct': 'Court',\n",
    "                  'Dr': 'Drive',\n",
    "                  'HIghway': 'Highway',\n",
    "                  'Hwy': 'Highway',\n",
    "                  'Pl': 'Place',\n",
    "                  'place': 'Place',\n",
    "                  'Pkwy': 'Parkway',\n",
    "                  'Rd': 'Road',\n",
    "                  'rd.': 'Road',\n",
    "                  'Sq.': 'Square',\n",
    "                  'St': 'Street',\n",
    "                  'st': 'Street',\n",
    "                  'ST': 'Street',\n",
    "                  'St,': 'Street',\n",
    "                  'St.': 'Street',\n",
    "                  'street': 'Street',\n",
    "                  'Street.': 'Street'\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's print the problematic names. We will not print all of them (we don't need to print the `Ave` or the `St`. So we're going to print only the whole lines where the total of the type is less than 20 and don't yet appear in expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#12': ['Harvard St #12'],\n",
       " '#1302': ['Cambridge Street #1302'],\n",
       " '#501': ['Bromfield Street #501'],\n",
       " '104': ['Mill Street, Suite 104'],\n",
       " '1100': ['First Street, Suite 1100'],\n",
       " '1702': ['Franklin Street, Suite 1702'],\n",
       " '3': ['Kendall Square - 3'],\n",
       " '303': ['First Street, Suite 303'],\n",
       " '6': ['South Station, near Track 6'],\n",
       " '846028': ['PO Box 846028'],\n",
       " 'Albany': ['Albany', 'Albany', 'Albany'],\n",
       " 'Boylston': ['Boylston', 'Boylston'],\n",
       " 'Brook': ['Furnace Brook'],\n",
       " 'Building': ['South Market Building'],\n",
       " 'Cambrdige': ['Cambrdige', 'Cambrdige'],\n",
       " 'Center': ['Cambridge Center',\n",
       "  'Channel Center',\n",
       "  'Financial Center',\n",
       "  'Cambridge Center',\n",
       "  'Cambridge Center',\n",
       "  'Channel Center',\n",
       "  'Cambridge Center',\n",
       "  'Channel Center',\n",
       "  'Channel Center',\n",
       "  'Cambridge Center',\n",
       "  'Channel Center',\n",
       "  'Cambridge Center',\n",
       "  'Cambridge Center',\n",
       "  'Channel Center',\n",
       "  'Channel Center'],\n",
       " 'Circle': ['Norcross Circle',\n",
       "  'Norcross Circle',\n",
       "  'Norcross Circle',\n",
       "  'Achorn Circle',\n",
       "  'Achorn Circle',\n",
       "  'Stein Circle',\n",
       "  'Edgewood Circle'],\n",
       " 'Corner': ['Webster Street, Coolidge Corner'],\n",
       " 'Dartmouth': ['Dartmouth'],\n",
       " 'Driveway': ['Museum of Science Driveway'],\n",
       " 'Elm': ['Elm'],\n",
       " 'Ext': ['B Street Ext'],\n",
       " 'Fellsway': ['Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway',\n",
       "  'Fellsway'],\n",
       " 'Fenway': ['Fenway', 'Fenway', 'Fenway', 'Fenway', 'Fenway'],\n",
       " 'Floor': ['Boylston Street, 5th Floor'],\n",
       " 'Garage': ['Stillings Street Garage'],\n",
       " 'Greenway': ['East Boston Greenway', 'East Boston Greenway'],\n",
       " 'H': ['H'],\n",
       " 'Hall': ['Faneuil Hall'],\n",
       " 'Hampshire': ['Hampshire'],\n",
       " 'Highway': ['Cummins Highway',\n",
       "  'Providence Highway',\n",
       "  \"Monsignor O'Brien Highway\",\n",
       "  \"Monsignor O'Brien Highway\",\n",
       "  'Santilli Highway',\n",
       "  \"Monsignor O'Brien Highway\",\n",
       "  \"Monsignor O'Brien Highway\",\n",
       "  'Santilli Highway',\n",
       "  \"Monsignor O'Brien Highway\",\n",
       "  'Santilli Highway',\n",
       "  'American Legion Highway',\n",
       "  'American Legion Highway',\n",
       "  'Santilli Highway'],\n",
       " 'Holland': ['Holland'],\n",
       " 'Jamaicaway': ['Jamaicaway', 'Jamaicaway'],\n",
       " 'LEVEL': ['LOMASNEY WAY, ROOF LEVEL'],\n",
       " 'Lafayette': ['Avenue De Lafayette'],\n",
       " 'Longwood': ['Longwood'],\n",
       " 'Mall': ['Cummington Mall', 'Cummington Mall', 'Cummington Mall'],\n",
       " 'Market': ['Faneuil Hall Market'],\n",
       " 'Newbury': ['Newbury', 'Newbury'],\n",
       " 'Pasteur': ['Avenue Louis Pasteur',\n",
       "  'Avenue Louis Pasteur',\n",
       "  'Avenue Louis Pasteur'],\n",
       " 'Plaza': ['Two Center Plaza', 'Park Plaza'],\n",
       " 'Row': ['Assembly Row',\n",
       "  'Assembly Row',\n",
       "  'Assembly Row',\n",
       "  'Professors Row',\n",
       "  'East India Row',\n",
       "  'Assembly Row'],\n",
       " 'South': ['Charles Street South', 'Charles Street South'],\n",
       " 'Windsor': ['Windsor', 'Windsor'],\n",
       " 'Winsor': ['Winsor'],\n",
       " 'floor': ['Sidney Street, 2nd floor', 'First Street, 18th floor']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typo_full_names = {}\n",
    "\n",
    "def audit_street_name(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if (all_types[street_type] < 20) and (street_type not in expected) and (street_type not in abbr_mapping):\n",
    "            if street_type in typo_full_names:\n",
    "                typo_full_names[street_type].append(street_name)\n",
    "            else:\n",
    "                typo_full_names.update({ street_type:[street_name] })\n",
    "\n",
    "def audit_name(filename):\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_name(street_types, elem.attrib['v'])    \n",
    "    # print_sorted_dict(street_types)\n",
    "    return typo_full_names\n",
    "\n",
    "audit_name(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define other mapping dictionaries and apply new, individual corrections based on the information we have.\n",
    "\n",
    "1. The numbers we see are either:\n",
    " - house numbers, that belong to the tag `addr:housenumber`\n",
    " - suite numbers, that belong to the tag `addr:suitenumber`\n",
    " - train track numbers, that we don't necessarily need (knowing where to find South Station is sufficient)\n",
    " - a PO Box that happens to be on Albany Street. The `PO Box 846028` information appears in the `addr:housenumber` tag already  \n",
    "2. `Albany`, `Boylston`, `Cambridge` (written `Cambrdige` here with a typo), `Dartmouth`, `Elm`, `Hampshire`, `Holland`, `Newbury` are Boston street names\n",
    "3. `Lafayette`, `Longwood` and `Winsor` are Boston avenue names\n",
    "4. `Pasteur` corresponds to `Avenue Louis Pasteur`\n",
    "5. The `South Market Building` is located on 4 South Market Street\n",
    "6. All `Center`s are correct, we will add `Center` to our `expected` list\n",
    "7. All `Circle`s are correct, we will add `Circle` to our `expected` list\n",
    "8. `Coolidge Corner` should not appear here\n",
    "9. `Museum of Science Driveway` is correct, we will add `Driveway` to our `expected` list\n",
    "10. `B Street Ext` corresponds to `B Street`\n",
    "11. `Fellsway` is a parkway\n",
    "12. `Fenway` is of course Fenway Park\n",
    "13. `Boylston Street, 5th Floor` should only be `Boylston Street`, the `5th Floor` information should appear in the tag `addr:floornumber`\n",
    "14. `Stillings Street Garage` is a garage located on 11 Stillings Street\n",
    "15. `East Boston Greenway` is correct\n",
    "16. `H` is actually 605 Hancock Street\n",
    "17. `Faneuil Hall` is located 4 South Market Street\n",
    "18. `Jamaicaway` is correct\n",
    "19. `LOMASNEY WAY, ROOF LEVEL` should only be `Lomasney Way`\n",
    "20. `Cummington Mall` is correct, we will add `Mall` to our `expected` list\n",
    "21. `Faneuil Hall Market` is 1 Faneuil Hall Square\n",
    "22. `Two Center Plaza` should be 2 Center Plaza\n",
    "23. `Park Plaza` is correct\n",
    "24. `Charles Street South` should simply be `Charles Street`\n",
    "25. `Sidney Street, 2nd floor` should only be `Sidney Street`, the `2nd Floor` information should appear in the tag `addr:floornumber`\n",
    "26. `First Street, 18 floor` should only be `First Street`, the `18th Floor` information should appear in the tag `addr:floornumber`\n",
    "27. `Windsor` corresponds to `Windsor Place` in Sommerville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artery',\n",
       " 'Avenue',\n",
       " 'Boulevard',\n",
       " 'Broadway',\n",
       " 'Center',\n",
       " 'Circle',\n",
       " 'Commons',\n",
       " 'Court',\n",
       " 'Drive',\n",
       " 'Driveway',\n",
       " 'Lane',\n",
       " 'Mall',\n",
       " 'Park',\n",
       " 'Parkway',\n",
       " 'Place',\n",
       " 'Road',\n",
       " 'Square',\n",
       " 'Street',\n",
       " 'Terrace',\n",
       " 'Trail',\n",
       " 'Turnpike',\n",
       " 'Wharf',\n",
       " 'Yard']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected.extend(['Center', 'Circle', 'Driveway', 'Mall'])\n",
    "\n",
    "typo_mapping = {  'Albany': 'Albany Street',\n",
    "                  'Boylston': 'Boylston Street',\n",
    "                  'Cambrdige': 'Cambridge Street',\n",
    "                  'Dartmouth': 'Dartmouth Street',\n",
    "                  'Elm': 'Elm Street',\n",
    "                  'Hampshire': 'Hampshire Street',\n",
    "                  'Holland': 'Holland Street',\n",
    "                  'Newbury': 'Newbury Street',\n",
    "                  'Lafayette': 'Lafayette Avenue',\n",
    "                  'Longwood': 'Longwood Avenue',\n",
    "                  'Winsor': 'Winsor Avenue',\n",
    "                  'Pasteur': 'Avenue Louis Pasteur',\n",
    "                  'Corner': 'Webster Street',\n",
    "                  'Building': {'South Market Street': 4},\n",
    "                  'B Street Ext': 'B Street',\n",
    "                  'Fellsway': 'Fellsway Parkway',\n",
    "                  'Fenway': 'Fenway Park',\n",
    "                  'Floor': {'Boylston Street': '5th Floor'},\n",
    "                  'Garage': 'Stillings Street',\n",
    "                  'H': 'Hancock Street',\n",
    "                  'Hall': {'Faneuil Hall Square': 1},\n",
    "                  'LEVEL': {'Lomasney Way': 'Roof Level'},\n",
    "                  'Market': 'Faneuil Hall Square',\n",
    "                  'Plaza': { 'Two Center Plaza': {'Center Plaza': 2}},\n",
    "                  'South': 'Charles Street South',\n",
    "                  'floor': { 'Sidney Street': '2nd Floor',\n",
    "                             'First Street': '18th Floor'},\n",
    "                  'Windsor': 'Windsor Place'\n",
    "               }\n",
    "\n",
    "numbers_mapping = { '#12': {'Harvard Street': 12},\n",
    "                    '#1302': {'Cambridge Street': 1302},\n",
    "                    '#501': {'Bromfield Street': 501},\n",
    "                    '104': {'Mill Street': 'Suite 104'},\n",
    "                    '1100': {'First Street': 'Suite 1100'},\n",
    "                    '1702': { 'Franklin Street': 'Suite 1702'},\n",
    "                    '3': {'Kendall Square': 'Suite B3201'},\n",
    "                    '303': {'First Street': 'Suite 303'},\n",
    "                    '6': {'Atlantic Avenue': 700 }\n",
    "                  }\n",
    "\n",
    "po_box_mapping = { '846028': 'Albany Street'}\n",
    "\n",
    "expected = sorted(expected)\n",
    "expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before running updates, let's see if any street name contains incorrect characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walnut St,', \"Monsignor O'Brien Hwy\", \"Monsignor O'Brien Highway\", 'Boylston Street, 5th Floor', 'Franklin Street, Suite 1702', 'South Station, near Track 6', 'LOMASNEY WAY, ROOF LEVEL', 'Webster Street, Coolidge Corner', \"Monsignor O'Brien Highway\", 'First Street, Suite 303', 'Sidney Street, 2nd floor', 'First Street, 18th floor', 'First Street, Suite 1100', 'Cambridge Street #1302', 'Harvard St #12', 'Bromfield Street #501', 'Mill Street, Suite 104', \"Monsignor O'Brien Highway\", \"Monsignor O'Brien Highway\", \"Saint Mary's St.\", \"Monsignor O'Brien Highway\", \"River's Edge Drive\", 'Church Street, Harvard Square', 'Massachusetts Ave; Mass Ave', \"St. Paul's Ave\"]\n"
     ]
    }
   ],
   "source": [
    "name_problem_chars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\t\\r\\n]')\n",
    "\n",
    "def get_problem_names(filename):\n",
    "    \"\"\"\n",
    "    Takes in a dataset in XML format, parses it and returns a list with the values of tags with problematic characters.\n",
    "    \"\"\"\n",
    "    problemchars_list = []\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        if is_street_name(element):\n",
    "            if name_problem_chars.search(element.attrib['v']):\n",
    "                problemchars_list.append(element.attrib['v'])\n",
    "    return problemchars_list\n",
    "\n",
    "print(get_problem_names(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all of these entries are correct. We have two entries we need to investigate:\n",
    "   - `Church Street, Harvard Square` corresponds to a parish located on 1446 Massachusetts Avenue\n",
    "   - `Massachusetts Ave; Mass Ave` should only be `Massachusetts Avenue`\n",
    "   \n",
    "We will define one last dictionary to correct them, and then we will finally correct the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_mapping = { 'Church Street, Harvard Square': {'Massachusetts Avenue': 1446},\n",
    "                 'Massachusetts Ave; Mass Ave': 'Massachusetts Avenue'\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have identified all the incorrect names. We created dictionaries to update incorrect values. We can now clean the data and save it all in a final dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Massachusetts Ave; Mass Ave\n",
      "1 Kendall Sq.: 1 Kendall Square\n",
      "738 Commonwealth Ave: 738 Commonwealth Avenue\n",
      "Abby Rd: Abby Road\n",
      "Aberdeen Rd: Aberdeen Road\n",
      "Adams St: Adams Street\n",
      "Albany: Albany Street\n",
      "Albion St.: Albion Street\n",
      "American Legion HIghway: American Legion Highway\n",
      "Antwerp St: Antwerp Street\n",
      "argus place: argus Place\n",
      "Arsenal St: Arsenal Street\n",
      "Athol St: Athol Street\n",
      "Avenue De Lafayette: Lafayette Avenue\n",
      "Bagnal St: Bagnal Street\n",
      "Banks St.: Banks Street\n",
      "Birmingham Pkwy: Birmingham Parkway\n",
      "Blue Hill Ave: Blue Hill Avenue\n",
      "Boston Ave: Boston Avenue\n",
      "Boston street: Boston Street\n",
      "Bowdoin St: Bowdoin Street\n",
      "Boylston: Boylston Street\n",
      "Boylston St.: Boylston Street\n",
      "Boylston Street, 5th Floor: Boylston Street\n",
      "Brentwood St: Brentwood Street\n",
      "Brighton Ave.: Brighton Avenue\n",
      "Bristol Rd: Bristol Road\n",
      "Broad St: Broad Street\n",
      "Bromfield Street #501: Bromfield Street\n",
      "Cambrdige: Cambridge Street\n",
      "Cambridge St: Cambridge Street\n",
      "Cambridge Street #1302: Cambridge Street\n",
      "Centre St: Centre Street\n",
      "Centre St.: Centre Street\n",
      "Charles St: Charles Street\n",
      "College Ave: College Avenue\n",
      "Commonwealth Ave: Commonwealth Avenue\n",
      "Concord Ave: Concord Avenue\n",
      "Congress St: Congress Street\n",
      "Corey rd.: Corey Road\n",
      "Court St: Court Street\n",
      "Cummington St: Cummington Street\n",
      "Dane St: Dane Street\n",
      "Dartmouth: Dartmouth Street\n",
      "Duval St: Duval Street\n",
      "Elm: Elm Street\n",
      "Elm St: Elm Street\n",
      "Elm St.: Elm Street\n",
      "Everett Ave: Everett Avenue\n",
      "Everett St: Everett Street\n",
      "Faneuil Hall: Faneuil Hall Square\n",
      "Faneuil Hall Market: Faneuil Hall Square\n",
      "Fellsway: Fellsway Parkway\n",
      "Fenway: Fenway Park\n",
      "First Street, 18th floor: First Street\n",
      "First Street, Suite 1100: First Street\n",
      "First Street, Suite 303: First Street\n",
      "Francesca Ave: Francesca Avenue\n",
      "Franklin Street, Suite 1702: Franklin Street\n",
      "George St: George Street\n",
      "Goodnough Rd: Goodnough Road\n",
      "Grove St: Grove Street\n",
      "H: Hancock Street\n",
      "Hammond St: Hammond Street\n",
      "Hampshire: Hampshire Street\n",
      "Hampshire St: Hampshire Street\n",
      "Hancock Street.: Hancock Street\n",
      "Harborside Dr: Harborside Drive\n",
      "Harrison Ave: Harrison Avenue\n",
      "Harvard St #12: Harvard Street\n",
      "Highland Ave: Highland Avenue\n",
      "Holland: Holland Street\n",
      "Holton St: Holton Street\n",
      "Josephine Ave: Josephine Avenue\n",
      "Kelley Ct: Kelley Court\n",
      "Kendall Square - 3: Kendall Square\n",
      "Kirkland St: Kirkland Street\n",
      "Leighton St: Leighton Street\n",
      "Lexington Ave: Lexington Avenue\n",
      "Litchfield St: Litchfield Street\n",
      "LOMASNEY WAY, ROOF LEVEL: Lomasney Way\n",
      "Longfellow Pl: Longfellow Place\n",
      "Longwood: Longwood Avenue\n",
      "Lothrop St: Lothrop Street\n",
      "Mackin St: Mackin Street\n",
      "Main St: Main Street\n",
      "Main st: Main Street\n",
      "Main St.: Main Street\n",
      "Marshall St.: Marshall Street\n",
      "Massachusetts Ave: Massachusetts Avenue\n",
      "Massachusetts Ave.: Massachusetts Avenue\n",
      "Massachusetts Ave; Mass Ave: Massachusetts Avenue\n",
      "Maverick St: Maverick Street\n",
      "Maverick St.: Maverick Street\n",
      "Medford St: Medford Street\n",
      "Merrill St: Merrill Street\n",
      "Mill Street, Suite 104: Mill Street\n",
      "Monsignor O'Brien Hwy: Monsignor O'Brien Highway\n",
      "Morrison Ave: Morrison Avenue\n",
      "Mt Auburn St: Mt Auburn Street\n",
      "Mystic Ave: Mystic Avenue\n",
      "N Beacon St: N Beacon Street\n",
      "Newbury: Newbury Street\n",
      "Newton ST: Newton Street\n",
      "Norfolk St: Norfolk Street\n",
      "Oakland Rd: Oakland Road\n",
      "Park Plaza: Two Center Plaza\n",
      "Pearl St.: Pearl Street\n",
      "PO Box 846028: Albany Street\n",
      "Portsmouth St: Portsmouth Street\n",
      "Prospect St.: Prospect Street\n",
      "Rawson Rd: Rawson Road\n",
      "Richardson St: Richardson Street\n",
      "Sagamore Ave: Sagamore Avenue\n",
      "Saint Mary's St.: Saint Mary's Street\n",
      "Sea St: Sea Street\n",
      "Sidney Street, 2nd floor: Sidney Street\n",
      "Soldiers Field Rd: Soldiers Field Road\n",
      "Somerville Ave: Somerville Avenue\n",
      "Somerville Ave.: Somerville Avenue\n",
      "South Market Building: South Market Street\n",
      "South Station, near Track 6: Atlantic Avenue\n",
      "South Waverly St: South Waverly Street\n",
      "Spaulding Ave.: Spaulding Avenue\n",
      "Squanto Rd: Squanto Road\n",
      "St. Paul's Ave: St. Paul's Avenue\n",
      "Stewart St: Stewart Street\n",
      "Stillings Street Garage: Stillings Street\n",
      "Stuart St.: Stuart Street\n",
      "Tremont St.: Tremont Street\n",
      "Two Center Plaza: Center Plaza\n",
      "Walnut St,: Walnut Street\n",
      "Ware St: Ware Street\n",
      "Washington Ave: Washington Avenue\n",
      "Waverly St: Waverly Street\n",
      "Webster Street, Coolidge Corner: Webster Street\n",
      "Western Ave: Western Avenue\n",
      "Willow Ave: Willow Avenue\n",
      "Windsor: Windsor Place\n",
      "Winsor: Winsor Avenue\n",
      "Winter St: Winter Street\n",
      "Winthrop St: Winthrop Street\n"
     ]
    }
   ],
   "source": [
    "def typo_correct(street_name, street_type):\n",
    "    if type(typo_mapping[street_type]) == type('string'):\n",
    "            name = typo_mapping[street_type]\n",
    "    elif type(typo_mapping[street_type]) == type({}):\n",
    "        if '2nd' in street_name:\n",
    "            name = 'Sidney Street'\n",
    "            # add attribute addr:floor '2nd Floor' typo_mapping[street_type]['Sidney Street']\n",
    "        elif '18' in street_name:\n",
    "            name = 'First Street'\n",
    "            # add attribute addr:floor '18th Floor' typo_mapping[street_type]['First Street']\n",
    "        elif '5th' in street_name:\n",
    "            name = 'Boylston Street'\n",
    "            # add attribute addr:floor '5th Floor' typo_mapping[street_type]['Boylston Street']\n",
    "        elif street_type == 'LEVEL':\n",
    "            name = 'Lomasney Way'\n",
    "            # add attribute addr:floor 'Roof Level' typo_mapping[street_type]['Lomasney Way']\n",
    "        elif 'Two Center' in street_name:\n",
    "            name = 'Center Plaza'\n",
    "            # add attribute addr:housenumber '2' typo_mapping[street_type]['Two Center Plaza']['Center Plaza']\n",
    "        else:\n",
    "            for key in typo_mapping[street_type]:\n",
    "                name = key\n",
    "                # add attribute addr:housenumber = value\n",
    "    return name\n",
    "        \n",
    "def numbers_correct(street_name, street_type):\n",
    "    if 'Suite' in street_name:\n",
    "        for key in numbers_mapping[street_type]:\n",
    "            name = key\n",
    "            # print (name)\n",
    "            # add attribute addr:suitenumber = value\n",
    "    else:\n",
    "        for key in numbers_mapping[street_type]:\n",
    "            name = key\n",
    "            # add attribute addr:housenumber = value\n",
    "    return name\n",
    "\n",
    "def char_correct(street_name, street_type):\n",
    "    print (street_name, street_type)\n",
    "    if street_name + ' ' + street_type == 'Church Street, Harvard Square':\n",
    "        for key in char_mapping[street_name + ' ' + street_type]:\n",
    "            name = key\n",
    "    elif street_name + ' ' + street_type == 'Massachusetts Ave; Mass Ave':\n",
    "        name = char_mapping[street_name + ' ' + street_type]\n",
    "    return name\n",
    "\n",
    "def audit_abbreviations(filename):\n",
    "    problem_street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(filename):\n",
    "        if is_street_name(elem):\n",
    "            expected_street_type(problem_street_types, elem.attrib['v'])\n",
    "    return problem_street_types\n",
    "\n",
    "def expected_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "        \n",
    "def update_name(name):\n",
    "    street_type = name.split(' ')[-1]\n",
    "    street_name = name.rsplit(' ', 1)[0]\n",
    "    if (street_name + ' ' + street_type) in char_mapping:\n",
    "        name = char_correct(street_name, street_type)\n",
    "    elif street_type in abbr_mapping:\n",
    "        name = street_name + ' ' + abbr_mapping[street_type]\n",
    "    elif street_type in typo_mapping:\n",
    "        name = typo_correct(street_name, street_type)\n",
    "    elif street_type in numbers_mapping:\n",
    "        name = numbers_correct(street_name, street_type)\n",
    "    \n",
    "    elif street_type in po_box_mapping:\n",
    "        name = po_box_mapping[street_type]\n",
    "    return name\n",
    "    \n",
    "def run_updates(filename):\n",
    "    st_types = audit_abbreviations(dataset)\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name)\n",
    "            if better_name != name:\n",
    "                corrected_names[name] = better_name\n",
    "    return corrected_names\n",
    "            \n",
    "corrected_names = {}           \n",
    "corrected_names = run_updates(dataset)\n",
    "print_sorted_dict(corrected_names, \"%s: %s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! We now have a dictionary of all the incorrect values and their correction. We're ready to prepare the data to be inserted into a SQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SCHEMA = {\n",
    "    'node': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'lat': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'lon': {'required': True, 'type': 'float', 'coerce': float},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'node_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way': {\n",
    "        'type': 'dict',\n",
    "        'schema': {\n",
    "            'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'user': {'required': True, 'type': 'string'},\n",
    "            'uid': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'version': {'required': True, 'type': 'string'},\n",
    "            'changeset': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "            'timestamp': {'required': True, 'type': 'string'}\n",
    "        }\n",
    "    },\n",
    "    'way_nodes': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'node_id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'position': {'required': True, 'type': 'integer', 'coerce': int}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'way_tags': {\n",
    "        'type': 'list',\n",
    "        'schema': {\n",
    "            'type': 'dict',\n",
    "            'schema': {\n",
    "                'id': {'required': True, 'type': 'integer', 'coerce': int},\n",
    "                'key': {'required': True, 'type': 'string'},\n",
    "                'value': {'required': True, 'type': 'string'},\n",
    "                'type': {'required': True, 'type': 'string'}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "\n",
    "import schema\n",
    "\n",
    "OSM_PATH = \"example.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def correct_element(v):\n",
    "    if v in corrected_names:\n",
    "        correct_value = corrected_names[v]\n",
    "    else:\n",
    "        correct_value = v\n",
    "    return correct_value\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        node_attribs['id'] = element.attrib['id']\n",
    "        node_attribs['user'] = element.attrib['user']\n",
    "        node_attribs['uid'] = element.attrib['uid']\n",
    "        node_attribs['version'] = element.attrib['version']\n",
    "        node_attribs['lat'] = element.attrib['lat']\n",
    "        node_attribs['lon'] = element.attrib['lon']\n",
    "        node_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        node_attribs['changeset'] = element.attrib['changeset']\n",
    "        \n",
    "        for node in element:\n",
    "            tag_dict = {}\n",
    "            tag_dict['id'] = element.attrib['id']\n",
    "            if ':' in node.attrib['k']:\n",
    "                tag_dict['type'] = node.attrib['k'].split(':', 1)[0]\n",
    "                tag_dict['key'] = node.attrib['k'].split(':', 1)[-1]\n",
    "                tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "            else:\n",
    "                tag_dict['type'] = 'regular'\n",
    "                tag_dict['key'] = node.attrib['k']\n",
    "                tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "            tags.append(tag_dict)\n",
    "            \n",
    "    elif element.tag == 'way':\n",
    "        way_attribs['id'] = element.attrib['id']\n",
    "        way_attribs['user'] = element.attrib['user']\n",
    "        way_attribs['uid'] = element.attrib['uid']\n",
    "        way_attribs['version'] = element.attrib['version']\n",
    "        way_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        way_attribs['changeset'] = element.attrib['changeset']\n",
    "        n = 0\n",
    "        for node in element:\n",
    "            if node.tag == 'nd':\n",
    "                way_dict = {}\n",
    "                way_dict['id'] = element.attrib['id']\n",
    "                way_dict['node_id'] = node.attrib['ref']\n",
    "                way_dict['position'] = n\n",
    "                n += 1\n",
    "                way_nodes.append(way_dict)\n",
    "            if node.tag == 'tag':\n",
    "                tag_dict = {}\n",
    "                tag_dict['id'] = element.attrib['id']\n",
    "                if ':' in node.attrib['k']:\n",
    "                    tag_dict['type'] = node.attrib['k'].split(':', 1)[0]\n",
    "                    tag_dict['key'] = node.attrib['k'].split(':', 1)[-1]\n",
    "                    tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "                else:\n",
    "                    tag_dict['type'] = 'regular'\n",
    "                    tag_dict['key'] = node.attrib['k']\n",
    "                    tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "                tags.append(tag_dict)\n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, str) else v) for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "    codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "    codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "    codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "    codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = csv.DictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = csv.DictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = csv.DictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = csv.DictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = csv.DictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow((el['way']))\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "                    \n",
    "process_map(dataset, validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/swwelch/f1144229848b407e0a5d13fcb7fbbd6f  \n",
    "https://discussions.udacity.com/t/help-cleaning-data/169833/84  \n",
    "https://gist.github.com/carlward/54ec1c91b62a5f911c42#file-sample_project-md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exploring our data with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Creating database on disk\n",
    "sqlite_file = 'boston.db'\n",
    "conn = sqlite3.connect(sqlite_file)\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''DROP TABLE IF EXISTS nodes''')\n",
    "c.execute('''DROP TABLE IF EXISTS nodes_tags''')\n",
    "c.execute('''DROP TABLE IF EXISTS ways''')\n",
    "c.execute('''DROP TABLE IF EXISTS ways_tags''')\n",
    "c.execute('''DROP TABLE IF EXISTS ways_nodes''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY_NODES = \"\"\"\n",
    "CREATE TABLE nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_NODES_TAGS = \"\"\"\n",
    "CREATE TABLE nodes_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_WAYS = \"\"\"\n",
    "CREATE TABLE ways (\n",
    "    id INTEGER NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_WAYS_TAGS = \"\"\"\n",
    "CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_WAYS_NODES = \"\"\"\n",
    "CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "c.execute(QUERY_NODES)\n",
    "c.execute(QUERY_NODES_TAGS)\n",
    "c.execute(QUERY_WAYS)\n",
    "c.execute(QUERY_WAYS_TAGS)\n",
    "c.execute(QUERY_WAYS_NODES)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data from our csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tables are created. We now need to import our csv files into them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('nodes.csv','rt') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db1 = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "    \n",
    "with open('nodes_tags.csv','rt') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db2 = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "    \n",
    "with open('ways.csv','rt') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db3 = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "    \n",
    "with open('ways_tags.csv','rt') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db4 = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "    \n",
    "with open('ways_nodes.csv','rt') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db5 = [(i['id'], i['node_id'], i['position']) for i in dr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db1)\n",
    "c.executemany(\"INSERT INTO nodes_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db2)\n",
    "c.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db3)\n",
    "c.executemany(\"INSERT INTO ways_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db4)\n",
    "c.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?, ?, ?);\", to_db5)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying our insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have the following information about our original data:\n",
    "- 1,939,872 nodes\n",
    "- 310,285 ways\n",
    "- 1,263 relations\n",
    "- 10,894 members\n",
    "- 2,335,749 nds\n",
    "- 1402 unique users  \n",
    "\n",
    "Let's check if our database corresponds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1939872,)]\n",
      "[(310285,)]\n"
     ]
    }
   ],
   "source": [
    "c.execute('SELECT COUNT(*) FROM nodes')\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)\n",
    "\n",
    "c.execute('SELECT COUNT(*) FROM ways')\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're good! Let's query away!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying our SQL database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who are the top 3 users who brought the more modifications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('crschmidt', 1198858), ('jremillard-massgis', 198009), ('OceanVortex', 84754)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT DISTINCT nodes.user, COUNT(*)\n",
    "FROM nodes\n",
    "GROUP BY nodes.uid\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 3;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which proportion of the database did the top 10 users contribute to build?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('crschmidt', 61.80088170765906), ('jremillard-massgis', 10.207322957391003), ('OceanVortex', 4.369051153890566), ('wambag', 4.077021576681348), ('morganwahl', 3.4048638260668747), ('MassGIS Import', 2.8826128734267003), ('ryebread', 2.6775477969680472), ('ingalls_imports', 1.4554053050922948), ('Ahlzen', 1.3437484535062107), ('mapper999', 0.6402999785552861)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT DISTINCT nodes.user, COUNT(*) * 100.0 / (SELECT COUNT(*) FROM nodes)\n",
    "FROM nodes\n",
    "GROUP BY nodes.uid\n",
    "ORDER BY (COUNT(*) * 100.0 / (SELECT COUNT(*) FROM nodes)) DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 streets that contain the more nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Washington Street', 253), ('Massachusetts Avenue', 157), ('Centre Street', 136), ('Broadway', 118), ('Beacon Street', 116), ('Cambridge Street', 97), ('Boylston Street', 87), ('Adams Street', 86), ('Blue Hill Avenue', 84), ('Northeast Corridor', 83)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT ways_tags.value, COUNT(*)\n",
    "FROM ways_tags\n",
    "WHERE ways_tags.key = 'name'\n",
    "AND ways_tags.type = 'regular'\n",
    "GROUP BY ways_tags.value\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, how many nodes does a way contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7.5277535169279854,)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT AVG(Count)\n",
    "FROM\n",
    "    (SELECT COUNT(*) as Count\n",
    "    FROM ways\n",
    "    JOIN ways_nodes\n",
    "    ON ways.id = ways_nodes.id\n",
    "    GROUP BY ways.id);\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 amenities in Boston?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bench', 1069), ('restaurant', 678), ('school', 499), ('bicycle_parking', 318), ('library', 276), ('place_of_worship', 275), ('cafe', 270), ('fast_food', 195), ('bicycle_rental', 139), ('post_box', 124)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT value, COUNT(*) as Count\n",
    "FROM nodes_tags\n",
    "WHERE key='amenity'\n",
    "GROUP BY value\n",
    "ORDER BY Count DESC\n",
    "LIMIT 10;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's definitely room to seat in Boston."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the religious landscape like in Boston?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('christian', 245), ('jewish', 8), ('unitarian_universalist', 2), ('buddhist', 1), ('muslim', 1)]\n"
     ]
    }
   ],
   "source": [
    "QUERY = '''\n",
    "SELECT nodes_tags.value, COUNT(*) as Count\n",
    "FROM nodes_tags \n",
    "JOIN\n",
    "    (SELECT DISTINCT(id)\n",
    "    FROM nodes_tags\n",
    "    WHERE value='place_of_worship') as Sub\n",
    "ON nodes_tags.id=Sub.id\n",
    "WHERE nodes_tags.key='religion'\n",
    "GROUP BY nodes_tags.value\n",
    "ORDER BY Count DESC;\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irish descendants enjoying a strong presence in Boston, it would have been great to have a distinction between catholics and protestants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QUERY = '''\n",
    "SELECT AVG(C)\n",
    "FROM\n",
    "    (SELECT COUNT(*)) as C\n",
    "    FROM nodes\n",
    "    JOIN ways\n",
    "    ON nodes.uid = ways.uid\n",
    "    GROUP BY nodes.uid);\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Further Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I focused this analysis on identifying irregular street names, cleaning them, and reformatting them so that one type corresponds to one term (Avenue, Street), getting rid of abbreviations.\n",
    "\n",
    "- To improve the cleaning result, the process should also include the insertion of tags. Some street names contained valuable information about floors, for example, or suite numbers, that deserve to be preserved. This process would enable us to present organized information at their specific place, preserving both the street names and the Floor or Suite numbers. \n",
    "\n",
    "- This data wrangling process should probably be pushed a step further in order to verify and clean zip codes as well, before being sent back to OpenStreetMaps. Zip codes could present similar issues than the ones encountered with street names:\n",
    "    - Too many or too few numbers\n",
    "    - Zip codes at the wring place (not in the right tag)\n",
    "    - Zip codes containing irregular characters\n",
    "    - Zip codes corresponding to another information that should be preserved but given elsewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for review\n",
    "\n",
    "- Is there a better way to create the tables, in one statement, with a `for` loop and a list like `table_list = ['nodes.csv', 'nodes_tags.csv', 'schema.py', 'ways.csv', 'ways_nodes.csv', 'ways_tags.csv']` ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
